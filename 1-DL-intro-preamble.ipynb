{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction into Deep Learning\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Andrey Ustyuzhanin<sup>1,2</sup>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "###### ICCUB School, 2016-10, Institute of Cosmos sciences, Barcelona\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### <sup>1</sup> Yandex School of Data Analysis,\n",
    "#### <sup>2</sup> Higher School of Economics\n",
    "<img src=\"imgs/YSDA_logo.png\" height=20>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## whoami, Yandex\n",
    "\n",
    "- A Dutch company (according to NASDAQ)\n",
    "- The leading web search engine in Russia\n",
    "- Image search\n",
    "- Speech recognition\n",
    "- Car traffic prediction\n",
    "- Mail and spam filtering\n",
    "- Natural language translation\n",
    "- Yandex Data Factory - data science for business\n",
    "- Yandex School of Data Analysis (YSDA)\n",
    "\n",
    "<!-- img src=\"http://www.marekrei.com/blog/wp-content/uploads/2016/01/CYh2GMnWkAELDTL.jpg\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## whoami, YSDA\n",
    "\n",
    "- Education:\n",
    "    - Curricula in Data & Computer Science\n",
    "    - Free tuition\n",
    "    - No employment obligations on part of the students (yet many go to Yandex)\n",
    "    - 500+ students graduated since 2007\n",
    "- Research\n",
    "    - Organizes Machine Learning Conference\n",
    "    - Interest in interdisciplinary research (eScience)\n",
    "    - A full member of LHCb and SHiP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "- understand Deep Learning landscape\n",
    "- understand basics, terminology\n",
    "- understand toolboxes/frameworks\n",
    "- get hands-on experience with some problems\n",
    "- know where to look for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why deep learning\n",
    "\n",
    "  - image recognition\n",
    "  - text recognition\n",
    "  - voice recognition\n",
    "  - Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Physics examples\n",
    "\n",
    "- Higgs boson exotic decay (ATLAS, simulated dataset)\n",
    "- Jet tagging (CMS)\n",
    "- muon track identification (CRAYFIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Not so long time ago, 1965\n",
    "<img src=https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/12/GMDH-network.png>\n",
    "<small>The achitecture of the first known deep network which was trained by Alexey Grigorevich Ivakhnenko in 1965. The feature selection steps after every layer lead to an ever-narrowing architecture which terminates when no further improvement can be achieved by the addition of another layer. Image of Prof. Alexey Ivakhnenko courtesy of Wikipedia.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Short history, continued\n",
    "\n",
    "- The earliest convolutional networks were used by Fukushima, 1979\n",
    "- Backpropagation in the modern form was derived first by Linnainmaa, 1970\n",
    "- Rumelhart, Hinton, and Williams, 1985 backpropagation in neural networks could yield interesting representations\n",
    "- LeCunn, 1989 convolutional networks + backpropagation = LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/https://www.youtube.com/watch?v=FwFduRA_L6Q&feature=youtu.be\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fc0642ec690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"https://www.youtube.com/watch?v=FwFduRA_L6Q&feature=youtu.be\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## a few more milestones\n",
    "\n",
    "- Schmidhuber 1992, pretraining approaches for RNN\n",
    "- Schmidhuber and Hochreiter in 1997, LSTM (Long-Short Term Memory)\n",
    "- 2011, 2012 Ciresan et al, won character recognition, traffic sign, medical imaging competitions with convolutional architecture\n",
    "- Krizhevsky, Sutskever, Hinton, 2012, CNN + ReLU + dropout = won ImageNet competition (AlexNet)\n",
    "- Google, Facebook, Microsoft made major acquisitions in 2012-2014 of deep learning startups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "$p_i = \\sigma(\\sum_k X_{ik} w_k)$\n",
    "\n",
    "$\\text{llh}=\\sum_i y_i \\log{p_i} + (1-y_i)\\log{(1 - p_i)}\\qquad$  (here $y \\in \\{0, 1\\}$)\n",
    "\n",
    "$loss = -\\text{llh}, \\qquad loss \\to \\min$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artificial neuron, unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"imgs/Perceptron.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"imgs/activation.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular activation functions\n",
    "\n",
    "* **Sigmoid**:\n",
    "\n",
    "    $f(x) = \\frac{1}  {1+e^{-x}}$\n",
    "\n",
    "\n",
    "* **ReLU - rectifier linear unit**\n",
    "\n",
    "    In the context of artificial neural networks, the rectifier is an activation function defined as\n",
    "\n",
    "    $f(x) = \\max(0, x)$\n",
    "\n",
    "    (argued to be more biologically plausible than the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression)). The most popular activation function for deep neural networks.\n",
    "\n",
    "* **Softplus**\n",
    "    A smooth approximation to the rectifier is the analytic function\n",
    "\n",
    "    $f(x) = \\ln(1 + e^x)$\n",
    "\n",
    "    which is called the softplus function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Layer\n",
    "\n",
    "Layer – a building block for NNs :\n",
    "- “Dense layer”: $f(x) = Wx+b$\n",
    "- “Nonlinearity layer”: $f(x) = σ(x)$\n",
    "- Input layer, output layer\n",
    "- A few more we gonna cover later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"imgs/MLP.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation (aka backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method for finding the gradient of the error with respect to weights over a neural network. The gradient signifies how the error of the network changes with changes to the network’s weights.\n",
    "<img  src=\"imgs/backprop-step.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"imgs/backprop.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"imgs/optimizers.gif\" width=\"80%\"/>\n",
    "\n",
    "<small> Behavior of different methods to accelerate gradient descent on a saddle point. Saddle points are thought to be the main difficulty in optimizing deep networks. Image by Alec Radford.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Motivations for depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Much has been studied about the depth of neural nets. Is has been proven mathematically[1] and empirically that convolutional neural network benifit from depth! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] - On the Expressive Power of Deep Learning: A Tensor Analysis - Cohen, et al 2015"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
